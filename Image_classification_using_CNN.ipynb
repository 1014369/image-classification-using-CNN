{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** What is a Convolutional Neural Network (CNN), and how does it differ from\n",
        "traditional fully connected neural networks in terms of architecture and performance on\n",
        "image data?\n",
        "- A Convolutional Neural Network (CNN) is a neural network designed for grid-like data, such as images whereas fully connected networks that flatten images into vectors, CNNs preserve the spatial structure by using convolutional layers with small filters.\n",
        "\n",
        "- **Architectural differnce**:CNNs include convolutional layers for feature extraction and pooling layers to reduce dimensions and introduce translation invariance. Fully connected networks lack this hierarchical feature learning and treat all inputs equally, making them inefficient for large images.\n",
        "\n",
        "- **Performance-wise**: CNNs are far superior on image data.CNN an detect edges, shapes, and complex patterns hierarchically, handle larger images efficiently, and generalize better due to fewer parameters much better than fully connected neural networks."
      ],
      "metadata": {
        "id": "_VhP-Cu19Ras"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Discuss the architecture of LeNet-5 and explain how it laid the foundation\n",
        "for modern deep learning models in computer vision. Include references to its original\n",
        "research paper.\n",
        "- It processes 32×32 grayscale images through a series of layers of convolutional layers extract local features, pooling layers reduce spatial dimensions and add translation invariance. It has a total of 7 layers.\n",
        "**Foundation for deep learning model**:LeNet-5 introduced has features like local receptive fields, weight sharing, hierarchical feature extraction, and spatial pooling, which are central to all modern CNN architectures. These principles made training deep networks on image data feasible and effective.\n",
        "- **Refernece**:Reference:\n",
        "LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324."
      ],
      "metadata": {
        "id": "McORJDJ39ak9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** Compare and contrast AlexNet and VGGNet in terms of design principles,\n",
        "number of parameters, and performance. Highlight key innovations and limitations of\n",
        "each.\n",
        "- AlexNet was designed to handle large-scale image classification (ImageNet) and popularized deep CNNs. It uses 8 layers (5 convolutional, 3 fully connected) with ReLU activations and dropout to reduce overfitting. VGGNet emphasizes simplicity and depth by stacking small 3×3 convolutional filters repeatedly, resulting in very deep networks (16 or 19 layers) with uniform design.\n",
        "\n",
        "- **Number of Parameters:**\n",
        "AlexNet has around 60 million parameters, largely due to its large fully connected layers. VGGNet has 138 million parameters (VGG-16), because of the deep stack of convolutional layers combined with fully connected layers, making it heavier and more memory-intensive.\n",
        "\n",
        "- **Performance:**\n",
        "AlexNet won the 2012 ImageNet competition, achieving a top-5 error of 15.3%, demonstrating the effectiveness of deep CNNs and GPUs for training. VGGNet improved accuracy further with a top-5 error of 7.3%,\n",
        "\n",
        "- **Key Innovations:**\n",
        "AlexNet introduced ReLU activations, dropout, and GPU-accelerated training for deep networks. VGGNet’s main innovation was using very small (3×3) convolution filters and increasing depth systematically to improve feature representation.\n",
        "\n",
        "- **Limitations:**\n",
        "AlexNet is less uniform and more ad-hoc in design, with relatively large filters and fewer layers, limiting feature richness. VGGNet is computationally expensive and memory-intensive, making it harder to deploy in resource-constrained environments."
      ],
      "metadata": {
        "id": "rv9n-RHe9gAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** What is transfer learning in the context of image classification? Explain\n",
        "how it helps in reducing computational costs and improving model performance with\n",
        "limited data.\n",
        "- **Transfer learning**:Transfer learning in image classification is a technique where a neural network trained on a large dataset, like ImageNet, is reused as a starting point for a different but related task. Instead of training a model from scratch, we take the pretrained network and either fine-tune it on your smaller dataset or use it as a fixed feature extractor.\n",
        "\n",
        "- **Cost**:This approach reduces computational costs because most of the network’s parameters are already learned, so you don’t need to perform full training.\n",
        "- **Performance**:It also improves performance on limited data because the model has already learned general features like edges and shapes which can be adapted to the new task, making it more accurate and robust even with fewer labeled examples.\n"
      ],
      "metadata": {
        "id": "gxnf8Tr89kh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** Describe the role of residual connections in ResNet architecture. How do\n",
        "they address the vanishing gradient problem in deep CNNs?\n",
        "- **Role of residual connection in ResNet**:Residual connections in ResNet are shortcut connections that skip one or more layers and directly add the input of a layer to its output. Instead of learning a full mapping, each residual block learns a residual function.This allows the network to focus on learning only the changes needed at each layer rather than the complete transformation.\n",
        "\n",
        "- **address the vanishing gradient problem in deep CNNs**:These connections address the vanishing gradient problem by providing a direct path for gradients to flow backward during training.Residual connections ensure that gradients can bypass certain layers, maintaining sufficient magnitude for effective weight updates and enabling training of extremely deep networks, sometimes exceeding hundreds of layers.\n"
      ],
      "metadata": {
        "id": "LawaRZiw9sHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:** Implement the LeNet-5 architectures using Tensorflow or PyTorch to\n",
        "classify the MNIST dataset. Report the accuracy and training time."
      ],
      "metadata": {
        "id": "5_pX5vYO9yTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(256, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LeNet5().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "start_time = time.time()\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}\")\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "print(f\"Training time: {training_time} seconds\")\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKlcEYqB-RVN",
        "outputId": "5ca97e4b-b268-4428-ce4c-e5e9a8e67cca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 53.0MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.62MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 13.4MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.12MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Batch 0/938, Loss: 2.2957444190979004\n",
            "Epoch 1/10, Batch 100/938, Loss: 0.18889173865318298\n",
            "Epoch 1/10, Batch 200/938, Loss: 0.262501060962677\n",
            "Epoch 1/10, Batch 300/938, Loss: 0.13307881355285645\n",
            "Epoch 1/10, Batch 400/938, Loss: 0.25817835330963135\n",
            "Epoch 1/10, Batch 500/938, Loss: 0.07323181629180908\n",
            "Epoch 1/10, Batch 600/938, Loss: 0.18460804224014282\n",
            "Epoch 1/10, Batch 700/938, Loss: 0.11794096976518631\n",
            "Epoch 1/10, Batch 800/938, Loss: 0.0990828275680542\n",
            "Epoch 1/10, Batch 900/938, Loss: 0.13926441967487335\n",
            "Epoch 2/10, Batch 0/938, Loss: 0.1396399736404419\n",
            "Epoch 2/10, Batch 100/938, Loss: 0.13415051996707916\n",
            "Epoch 2/10, Batch 200/938, Loss: 0.04058391973376274\n",
            "Epoch 2/10, Batch 300/938, Loss: 0.10102100670337677\n",
            "Epoch 2/10, Batch 400/938, Loss: 0.08717747032642365\n",
            "Epoch 2/10, Batch 500/938, Loss: 0.1171073168516159\n",
            "Epoch 2/10, Batch 600/938, Loss: 0.009262421168386936\n",
            "Epoch 2/10, Batch 700/938, Loss: 0.047096285969018936\n",
            "Epoch 2/10, Batch 800/938, Loss: 0.10298836976289749\n",
            "Epoch 2/10, Batch 900/938, Loss: 0.02525641955435276\n",
            "Epoch 3/10, Batch 0/938, Loss: 0.058170970529317856\n",
            "Epoch 3/10, Batch 100/938, Loss: 0.03948962688446045\n",
            "Epoch 3/10, Batch 200/938, Loss: 0.06093151122331619\n",
            "Epoch 3/10, Batch 300/938, Loss: 0.008057045750319958\n",
            "Epoch 3/10, Batch 400/938, Loss: 0.07368423044681549\n",
            "Epoch 3/10, Batch 500/938, Loss: 0.110652394592762\n",
            "Epoch 3/10, Batch 600/938, Loss: 0.11780897527933121\n",
            "Epoch 3/10, Batch 700/938, Loss: 0.029607290402054787\n",
            "Epoch 3/10, Batch 800/938, Loss: 0.03141471743583679\n",
            "Epoch 3/10, Batch 900/938, Loss: 0.04245292767882347\n",
            "Epoch 4/10, Batch 0/938, Loss: 0.0826059952378273\n",
            "Epoch 4/10, Batch 100/938, Loss: 0.017300531268119812\n",
            "Epoch 4/10, Batch 200/938, Loss: 0.03689771890640259\n",
            "Epoch 4/10, Batch 300/938, Loss: 0.07575813680887222\n",
            "Epoch 4/10, Batch 400/938, Loss: 0.03344652056694031\n",
            "Epoch 4/10, Batch 500/938, Loss: 0.04109129682183266\n",
            "Epoch 4/10, Batch 600/938, Loss: 0.041538361459970474\n",
            "Epoch 4/10, Batch 700/938, Loss: 0.029297402128577232\n",
            "Epoch 4/10, Batch 800/938, Loss: 0.23092250525951385\n",
            "Epoch 4/10, Batch 900/938, Loss: 0.005094841588288546\n",
            "Epoch 5/10, Batch 0/938, Loss: 0.036035750061273575\n",
            "Epoch 5/10, Batch 100/938, Loss: 0.015133615583181381\n",
            "Epoch 5/10, Batch 200/938, Loss: 0.05444018542766571\n",
            "Epoch 5/10, Batch 300/938, Loss: 0.014913409017026424\n",
            "Epoch 5/10, Batch 400/938, Loss: 0.010026424191892147\n",
            "Epoch 5/10, Batch 500/938, Loss: 0.017609285190701485\n",
            "Epoch 5/10, Batch 600/938, Loss: 0.04699729010462761\n",
            "Epoch 5/10, Batch 700/938, Loss: 0.006501658819615841\n",
            "Epoch 5/10, Batch 800/938, Loss: 0.010895732790231705\n",
            "Epoch 5/10, Batch 900/938, Loss: 0.003728361800312996\n",
            "Epoch 6/10, Batch 0/938, Loss: 0.028942318633198738\n",
            "Epoch 6/10, Batch 100/938, Loss: 0.0029228085186332464\n",
            "Epoch 6/10, Batch 200/938, Loss: 0.1707773208618164\n",
            "Epoch 6/10, Batch 300/938, Loss: 0.0021337405778467655\n",
            "Epoch 6/10, Batch 400/938, Loss: 0.040619343519210815\n",
            "Epoch 6/10, Batch 500/938, Loss: 0.0057449485175311565\n",
            "Epoch 6/10, Batch 600/938, Loss: 0.0033145693596452475\n",
            "Epoch 6/10, Batch 700/938, Loss: 0.011157822795212269\n",
            "Epoch 6/10, Batch 800/938, Loss: 0.08287406712770462\n",
            "Epoch 6/10, Batch 900/938, Loss: 0.08332736790180206\n",
            "Epoch 7/10, Batch 0/938, Loss: 0.03449435532093048\n",
            "Epoch 7/10, Batch 100/938, Loss: 0.029556594789028168\n",
            "Epoch 7/10, Batch 200/938, Loss: 0.0054526496678590775\n",
            "Epoch 7/10, Batch 300/938, Loss: 0.0030554146505892277\n",
            "Epoch 7/10, Batch 400/938, Loss: 0.027321703732013702\n",
            "Epoch 7/10, Batch 500/938, Loss: 0.021887794137001038\n",
            "Epoch 7/10, Batch 600/938, Loss: 0.007449643220752478\n",
            "Epoch 7/10, Batch 700/938, Loss: 0.03922436758875847\n",
            "Epoch 7/10, Batch 800/938, Loss: 0.0018251357832923532\n",
            "Epoch 7/10, Batch 900/938, Loss: 0.008852430619299412\n",
            "Epoch 8/10, Batch 0/938, Loss: 0.010566147975623608\n",
            "Epoch 8/10, Batch 100/938, Loss: 0.0031293542124330997\n",
            "Epoch 8/10, Batch 200/938, Loss: 0.0327904149889946\n",
            "Epoch 8/10, Batch 300/938, Loss: 8.03456932771951e-05\n",
            "Epoch 8/10, Batch 400/938, Loss: 0.0032050004228949547\n",
            "Epoch 8/10, Batch 500/938, Loss: 0.019563311710953712\n",
            "Epoch 8/10, Batch 600/938, Loss: 0.009458367712795734\n",
            "Epoch 8/10, Batch 700/938, Loss: 0.00043629901483654976\n",
            "Epoch 8/10, Batch 800/938, Loss: 0.0005544054438360035\n",
            "Epoch 8/10, Batch 900/938, Loss: 0.0005962325376458466\n",
            "Epoch 9/10, Batch 0/938, Loss: 0.09677977114915848\n",
            "Epoch 9/10, Batch 100/938, Loss: 0.004025767091661692\n",
            "Epoch 9/10, Batch 200/938, Loss: 0.02806316688656807\n",
            "Epoch 9/10, Batch 300/938, Loss: 0.0008903297130018473\n",
            "Epoch 9/10, Batch 400/938, Loss: 0.0010967639973387122\n",
            "Epoch 9/10, Batch 500/938, Loss: 0.0037009601946920156\n",
            "Epoch 9/10, Batch 600/938, Loss: 0.006242840085178614\n",
            "Epoch 9/10, Batch 700/938, Loss: 0.017417797818779945\n",
            "Epoch 9/10, Batch 800/938, Loss: 0.00984309520572424\n",
            "Epoch 9/10, Batch 900/938, Loss: 0.006798555143177509\n",
            "Epoch 10/10, Batch 0/938, Loss: 0.0007502103690057993\n",
            "Epoch 10/10, Batch 100/938, Loss: 0.0040394919924438\n",
            "Epoch 10/10, Batch 200/938, Loss: 0.06587380170822144\n",
            "Epoch 10/10, Batch 300/938, Loss: 0.00024087022757157683\n",
            "Epoch 10/10, Batch 400/938, Loss: 0.0011558355763554573\n",
            "Epoch 10/10, Batch 500/938, Loss: 0.0001623956923140213\n",
            "Epoch 10/10, Batch 600/938, Loss: 0.0016103204106912017\n",
            "Epoch 10/10, Batch 700/938, Loss: 0.00010649315663613379\n",
            "Epoch 10/10, Batch 800/938, Loss: 0.0017554316436871886\n",
            "Epoch 10/10, Batch 900/938, Loss: 0.010324413888156414\n",
            "Training time: 328.6437726020813 seconds\n",
            "Test accuracy: 98.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:** Use a pre-trained VGG16 model (via transfer learning) on a small custom\n",
        "dataset (e.g., flowers or animals). Replace the top layers and fine-tune the model.\n",
        "Include your code and result discussion."
      ],
      "metadata": {
        "id": "ne3idrg294uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "])\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "])\n",
        "train_data = datasets.ImageFolder('data/train', transform=train_transforms)\n",
        "val_data = datasets.ImageFolder('data/val', transform=val_transforms)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
        "num_classes = len(train_data.classes)\n",
        "model = models.vgg16(weights='IMAGENET1K_V1')\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(25088, 4096),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(4096, 1024),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(1024, num_classes)\n",
        ")\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.0005)\n",
        "epochs = 5\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += torch.sum(preds == labels).item()\n",
        "        total += labels.size(0)\n",
        "    train_acc = 100 * correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] Loss: {running_loss/len(train_loader):.4f} Train Acc: {train_acc:.2f}%\")\n",
        "training_time = time.time() - start_time\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += torch.sum(preds == labels).item()\n",
        "        total += labels.size(0)\n",
        "val_acc = 100 * correct / total\n",
        "print(f\"\\nValidation Accuracy: {val_acc:.2f}%\")\n",
        "print(f\"Training Time: {training_time:.2f} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ISOY6e66-SNZ",
        "outputId": "6b94d47c-bbc6-41a5-e0c4-f7ba3c5d2d0e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3326256453.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ])\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8:** Write a program to visualize the filters and feature maps of the first\n",
        "convolutional layer of AlexNet on an example input image."
      ],
      "metadata": {
        "id": "5ewGLxOJ-Ai_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "alexnet = models.alexnet(weights='IMAGENET1K_V1')\n",
        "alexnet.eval()\n",
        "img_path = 'sample.jpg'\n",
        "image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "input_tensor = transform(image).unsqueeze(0)\n",
        "first_conv = alexnet.features[0]\n",
        "weights = first_conv.weight.data.clone()\n",
        "weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(8):\n",
        "    plt.subplot(2, 4, i+1)\n",
        "    w = weights[i].permute(1, 2, 0).numpy()\n",
        "    plt.imshow(w)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Filter {i+1}')\n",
        "plt.suptitle(\"AlexNet - First Conv Layer Filters\", fontsize=14)\n",
        "plt.show()\n",
        "\n",
        "with torch.no_grad():\n",
        "    feature_maps = first_conv(input_tensor)\n",
        "\n",
        "feature_maps = feature_maps.squeeze(0)\n",
        "\n",
        "feature_maps = (feature_maps - feature_maps.min()) / (feature_maps.max() - feature_maps.min())\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(8):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.imshow(feature_maps[i].cpu().numpy(), cmap='viridis')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Map {i+1}')\n",
        "plt.suptitle(\"AlexNet - First Conv Layer Feature Maps\", fontsize=14)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "3Oo1Q6f7-TGP",
        "outputId": "2166700c-ce8a-4dc2-b6a2-87ffb2d8f1d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sample.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2738684030.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0malexnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sample.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m transform = transforms.Compose([\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3513\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3514\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3515\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample.jpg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9:** Train a GoogLeNet (Inception v1) or its variant using a standard dataset\n",
        "like CIFAR-10. Plot the training and validation accuracy over epochs and analyze\n",
        "overfitting or underfitting."
      ],
      "metadata": {
        "id": "A9SX9cGk-C9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((96,96)),  # smaller than 224 for speed\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((96,96)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
        "\n",
        "model = torchvision.models.googlenet(weights='IMAGENET1K_V1', aux_logits=False)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5\n",
        "train_acc_list, val_acc_list = [], []\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    correct, total = 0, 0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data,1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted==labels).sum().item()\n",
        "    train_acc = 100*correct/total\n",
        "    train_acc_list.append(train_acc)\n",
        "\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data,1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted==labels).sum().item()\n",
        "    val_acc = 100*correct/total\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}] Train Acc: {train_acc:.2f}% Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f\"Training time: {training_time:.2f}s\")\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(range(1, epochs+1), train_acc_list, label='Train Accuracy')\n",
        "plt.plot(range(1, epochs+1), val_acc_list, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('GoogLeNet Fine-tuning on CIFAR-10')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "xFr6r44vMzde",
        "outputId": "371e03cd-73af-416e-b816-4301c8fd53a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The parameter 'aux_logits' expected value True but got False instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3695069677.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgooglenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'IMAGENET1K_V1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword_only_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36minner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweights_param\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_weights_arg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/googlenet.py\u001b[0m in \u001b[0;36mgooglenet\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"transform_input\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0m_ovewrite_named_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transform_input\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0m_ovewrite_named_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aux_logits\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0m_ovewrite_named_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"init_weights\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0m_ovewrite_named_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_classes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"categories\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36m_ovewrite_named_param\u001b[0;34m(kwargs, param, new_value)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"The parameter '{param}' expected value {new_value} but got {kwargs[param]} instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The parameter 'aux_logits' expected value True but got False instead."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** You are working in a healthcare AI startup. Your team is tasked with\n",
        "developing a system that automatically classifies medical X-ray images into normal,\n",
        "pneumonia, and COVID-19. Due to limited labeled data, what approach would you\n",
        "suggest using among CNN architectures discussed (e.g., transfer learning with ResNet\n",
        "or Inception variants)? Justify your approach and outline a deployment strategy for\n",
        "production use.\n",
        "- I would use a transfer learning with ResNet as These models are already trained on large datasets (ImageNet) and can extract robust features from images, which is crucial when labeled medical X-ray data is limited.\n",
        "- **Data Preparation**:Preprocess X-ray images by resizing, normalizing, and augmenting (contrast adjustments) to increase dataset diversity. This improves model generalization and mitigates overfitting due to small datasets.\n",
        "- **Model Training**:Freeze the initial convolutional layers of the pretrained model and retrain the top layers on your X-ray dataset. Optionally, gradually unfreeze deeper layers for fine-tuning. Use categorical cross-entropy as the loss and monitor metrics like accuracy, precision and F1-score for evaluation.\n",
        "- **Validation Strategy**:Validate the model using k-fold cross-validation or a separate validation set to ensure robustness of the model.\n",
        "- **Deployment** DEploy the model using streamlit or flask.\n",
        "- **Monitor**: monitor the data and change continuoulsy for beter performance."
      ],
      "metadata": {
        "id": "xNB6maQf-LpN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tWNAD3B--OMa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}